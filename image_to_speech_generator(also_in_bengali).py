# -*- coding: utf-8 -*-
"""image_to_speech_generator(Also in Bengali).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ShpMyaqXm0t-ngzHiba8DPKFlZqaTKQz
"""

import locale
locale.getpreferredencoding = lambda: "UTF-8"
from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer ,GPT2TokenizerFast
import torch
from PIL import Image

import torch
from PIL import Image
from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer, GPT2TokenizerFast
import matplotlib.pyplot as plt

# Load the model and tokenizer
model = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
feature_extractor = ViTFeatureExtractor.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
vit_tokenizer = AutoTokenizer.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
gpt_tokenizer = GPT2TokenizerFast.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

max_length = 50  # Adjust max_length for longer captions
num_beams = 4
gen_kwargs = {"max_length": max_length, "num_beams": num_beams, "early_stopping": True}  # Enable early stopping for beam search

def predict_step(image_paths):
    captions = []
    for image_path in image_paths:
        i_image = Image.open(image_path)
        if i_image.mode != "RGB":
            i_image = i_image.convert(mode="RGB")


        plt.imshow(i_image)

        plt.show()
        # Preprocess the image
        images = [i_image]
        pixel_values = feature_extractor(images=images, return_tensors="pt").pixel_values.to(device)

        # Generate captions
        with torch.no_grad():
            output_ids = model.generate(pixel_values, **gen_kwargs)

        # Decode the generated caption using GPT-2 tokenizer
        cap = gpt_tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]
        captions.append(cap)

    return captions

# Example usage
captions = predict_step(['/content/E_wiDbrUcAooDeX.jpg'])
for cap in captions:
    print("Generated Caption (GPT-2 Tokenizer):", cap)
    print("-" * 50)



print(cap)

pip install gtts

pip install deep_translator

from deep_translator import GoogleTranslator
target_language = "Bengali" # @param ["English", "Spanish", "French", "German", "Italian", "Portuguese", "Polish", "Turkish", "Russian", "Dutch", "Czech", "Arabic", "Chinese (Simplified)", "Hindi", "Bengali"] {allow-input: true}

# Mapping between full names and ISO 639-1 codes
language_mapping = {
    'Bengali': 'bn',
    'Hindi'  : 'hi',
    'English': 'en',
    'Spanish': 'es',
    'French': 'fr',
    'German': 'de',
    'Italian': 'it',
    'Portuguese': 'pt',
    'Polish': 'pl',
    'Turkish': 'tr',
    'Russian': 'ru',
    'Dutch': 'nl',
    'Czech': 'cs',
    'Arabic': 'ar',
    'Chinese (Simplified)': 'zh-cn'
}

target = language_mapping[target_language]


ok=GoogleTranslator(source='auto', target=target).translate(cap)
print(ok)

from gtts import gTTS
import os

def text_to_speech(text, lang='en'):
    tts = gTTS(text=text, lang=lang, slow=False)
    tts.save("output.mp3")
    os.system("/content/Recording (2).m4a")  # This command plays the generated audio using mpg321 player. You can use any audio player of your choice.

# Example usage:
text_to_speech(ok,'bn')

